from rouge_score import rouge_scorer

def calculate_metrics(generated_summary: str, reference_summary: str):
    """
    Calculates ROUGE scores to measure how well the generated summary 
    captures information from the reference text.
    
    Args:
        generated_summary (str): The SOAP note generated by the AI.
        reference_summary (str): The original patient dialogue (Ground Truth).
        
    Returns:
        dict: ROUGE score objects.
    """
    # Initialize Scorer
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    
    # Calculate scores
    scores = scorer.score(reference_summary, generated_summary)
    
    # Optional: Log to console for backend debugging
    print(f"ROUGE-1: {scores['rouge1'].fmeasure:.4f} | ROUGE-L: {scores['rougeL'].fmeasure:.4f}")
    
    return scores